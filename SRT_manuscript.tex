\documentclass[a4paper,12pt]{article}

\usepackage{amsmath}                                                                                     % for equation*
\usepackage{graphicx}                                                                     %\includegraphics
\usepackage{multirow}
\usepackage[square,comma]{natbib}
\usepackage{subfigure}
\usepackage{color}                                                                                             % useful in editing process
\usepackage{floatrow}                                                                                       % table captions on top
\usepackage{enumitem}                                                                                     % for the square bracket enumeration
\usepackage{authblk}
\usepackage{setspace}                                                                                      %  \doublespacing
\usepackage[top=1.0in, bottom=1.0in, left=1.0in, right=1.0in]{geometry} % controlling the margins

\doublespacing


\bibliographystyle{plos2015} 

\floatsetup[table]{capposition=top}


\begin{document}

\author[1,*]{\footnotesize K. M{\o}llersen} 
\author[2]{M. Zortea}
\author[3]{T. R. Schopf}
\author[4]{H. Kirchesch}
\author[2]{F. Godtliebsen}

\affil[1]{Department of Community Medicine, UiT The Arctic University of Norway, Troms{\o}, Norway}
\affil[2]{Department of Mathematics and Statistics, UiT The Arctic University of Norway, Troms{\o}, Norway}
\affil[3]{Norwegian Centre for E-health Research, University Hospital of North Norway, Troms{\o}, Norway}
\affil[4]{Private office, Venloer Stra{\ss}e 107, 50259 Pulheim, Germany}
\affil[*]{P.O. Box 6050 Langnes, 9037 Troms{\o}, Norway, Telephone: +47 97 78 39 40, E-mail: kajsa.mollersen@uit.no}



\title{Comparison of computer systems and ranking criteria for automatic melanoma detection in dermoscopic images}
\date{}

\maketitle

{\bf Keywords:} Melanoma detection; computer system; dermoscopy; image analysis; skin cancer; open challenge; decision support; performance measures

\begin{abstract}
%Describe the main objective(s) of the study
%Explain how the study was done, including any model organisms used, without methodological detail
%Summarize the most important results and their significance
%Not exceed 300 words

Melanoma is the deadliest form of skin cancer, and early detection is crucial for patient survival. 
Computer systems can assist in melanoma detection, but are not widespread in clinical practice. 
In 2016, an open challenge in classification of dermoscopic images of skin lesions was announced. 
Nevus Doctor, a computer system previously developed by the authors, was used to participate in the challenge.
We compare five different measures for diagnostic accuracy by analysing the resulting ranking of the computer systems in the challenge. 
The different measures are discussed from a clinical perspective. 

The unexpected small variation in diagnostic accuracy according to segmentation method is investigated through comparison of an automatic versus the semi-automatic/manual segmentation. 
A small set of similar classification algorithms are used to investigate the impact of classifier on the diagnostic accuracy. 

A training set of $900$ dermoscopic images with corresponding class labels was released for the challenge. 
An independent test set of $379$ images, of which $75$ were of melanomas, was used to rank the participants. 

Choice of performance measure had great impact on the ranking of computer systems. 
Systems that were ranked among the top three for one measure, dropped to the bottom half when changing performance measure. 
From a clinical perspective, the misclassification of a melanoma as benign has far greater cost than the misclassification of a benign lesion as malignant. 
For a computer system to have clinical impact, its performance should be measured in a clinical relevant way. 

The small impact of segmentation method on diagnostic accuracy either suggests that the automatic segmentation algorithms do not need improvement, or that improvement should be measured against diagnostic accuracy rather than resemblance to manual segmentation. 
The variability in diagnostic accuracy for different classifier algorithms was larger than the variability for segmentation methods, and suggests a focus for future investigations. 




%\noindent {\bf Background:} Melanoma is the deadliest form of skin cancer, and early detection is crucial for patient survival. Computer systems can assist in melanoma detection, but are not widespread in clinical practice. In 2016, an open challenge in classification of dermoscopic images of skin lesions was announced.  \\ 
%{\bf Methods:} A training set of $900$ images with corresponding class labels was released. An independent test set of $379$ images, of which $75$ were of melanomas, was used to rank the participants according to average precision. Other measures of performance were also released after the challenge. Nevus Doctor, a computer system for melanoma detection previously developed by the authors, was used to participate in the challenge.  \\
%{\bf Results:} Nevus Doctor was ranked in the lower half among all participants according to the average precision measure. When ranking the participants according to high-sensitivity measures, Nevus Doctor was ranked among the upper half. \\ 
%{\bf Conclusion:} A challenge with a large data set and independent test set is a major contribution to research on computer systems for melanoma detection, but the clinical impact is limited. Future challenges should have more clinically relevant information about the data set, and more clinically oriented ranking criteria. 
\end{abstract}

\newpage

\section{Introduction}

Melanoma is the deadliest form of skin cancer. The survival rate drops as the level of tumour invasion increases, and therefore early detection is crucial for the patient's survival \citep{AmericanCancerSociety2016Cancer, CancerRegistryofNorway2016Cancer}. Melanoma detection relies on visual inspection, which is challenging because melanomas often resemble benign skin lesions. 
An aid in melanoma detection is the dermoscope, a simple device consisting of a magnifying lens and surrounding light, which is commonly used by dermatologists to examine skin conditions. 
The dermoscope reveals structures in lesions not visible to the naked eye, and has proven to increase diagnostic accuracy of skin lesions when used by trained personnel \citep{Kittler2002Diagnostic}.

Computer systems for melanoma detection seek to correctly classify a lesion based on input and previous training. 
The input is typically a dermoscopic image, but other technologies such as multispectral imaging and Rahman spectroscopy are also used. 
See e.g., \cite{Fink2016Noninvasive} and \cite{Masood2013Computer} for an overview. 
Training relies on an available database of labelled lesion images. 

Research and development of computer systems for melanoma detection have increased in the past few decades, following the development of digital cameras and attachable dermoscopes. 
An overview can be found in \cite{Korotkov2012Computerized}. 
Despite the effort made in developing new systems, their use in clinical practice is not widespread. 
An explanation may be the lack of convincing reports on the systems' diagnostic accuracy, owing to small data sets and non-comparable results. 

Small data sets result in training and testing on the same data, often by incorrect use of cross-validation. 
This gives overly optimistic estimates of the test error \citep{Smialowski2010Pitfalls,Hastie2009Elements}, and the classification accuracy of the system drops dramatically when tested on an independent test set. 
In the studies that report independent testing, the test set is often too small for the results to be generalised. 
In addition, consecutively collection of images is crucial for a clinical-like dataset. 
Only two studies that we know of have more than $50$ melanomas in an independent test set of consecutively collected images \citep{Monheit2011Performance, Malvehy2014Clinical}. 

The diagnostic difficulty of the data set has significant impact on the reported diagnostic accuracy of the system, as discussed in \cite{Rosado2003Accuracy}, and illustrated in \cite{Malvehy2014Clinical} and \cite{Mollersen2015Computeraided}. 
The diagnostic difficulty of a data set depends on factors such as proportion of melanomas, exclusion of non-melanocytic lesions, median Breslow depth, etc. 
Therefore, different systems cannot be compared unless they are tested on the same data set. 
There are few studies where several systems are tested on the same set of lesions, with the exception of \cite{Perrinaud2007Can} and \cite{Mollersen2015Computeraided}. 

A sufficiently large, publicly available data set makes independent testing and comparison of diagnostic accuracy between systems possible. 
In 2013, the PH$^2$ data set was made publicly available \citep{Mendonca2013PH2}, and added important value to the research on computer systems for melanoma detection. 
Consisting of only $200$ images, it does not overcome the aforementioned obstacles of small data sets. 
A larger data set, the ISIC Data Archive (see Section~\ref{sec:Challenge}) is now publicly available. 
In 2016, an open challenge using images from the ISIC Data Archive was announced. 
Nevus Doctor, a computer system developed previously by the authors, was used to participate in this challenge.
The design of the challenge allows for a direct comparison of diagnostic accuracy among the submitted melanoma detection systems, since they have all been tested on the same independent test set. 
Even when systems are tested on the same test set, the question on how to measure the diagnostic accuracy arises. 
Summarising the performance of a system for the whole range of sensitivity and specificity values gives a different perspective than measuring the performance only for very high sensitivity values. 
Choice of measure is not trivial as different measures penalises misclassifications differently, and it can have huge impact on the ranking. 

The rest of the article is organised as follows: Section~\ref{sec:Challenge} describes the data, the challenge and the five performance measures. In addition, a brief description of Nevus Doctor is given. In Section~\ref{sec:Results}, the variations in ranking of the top systems are presented. An investigation into segmentation method and classifier algorithm is also found here. A discussion of the results is provided in Section~\ref{sec:Discussion}, together with some concluding remarks. 

\section{The challenge} \label{sec:Challenge}

The International Skin Image Collaboration (ISIC) (http://isdis.net/isic-project/) has created a public repository for dermoscopic skin lesion images (https://isic-archive.com/). 
The current dataset (12.01.2016) consists of $12,086$ images and is by far the largest data set of dermoscopic images available. 
Each image is annotated as `benign' or `malignant' according to histopathological diagnosis or expert consensus based on revision of the image. 
The images come from different sources, and hence are taken with different cameras and dermoscopes. 

As part of the International Symposium on Biomedical Imaging 2016, the open challenge {\it Skin Lesion Analysis towards Melanoma Detection} was hosted by ISIC. 
The challenge consisted of three sub challenges: Segmentation, Feature Extraction, and Classification. 
Details can be found in \cite{Gutman2016Skin}. 
The Classification sub challenge had two parts: Automatic segmentation, where only the images were released, and Manual segmentation, where a semi-automatic or manually obtained mask was also released for each image. 
There were $25$ participants in the Automatic segmentation part, and $18$ participants in the Manual segmentation part. 
Many participated in both parts. 

A set of $900$ dermoscopic images of skin lesions with corresponding class labels and masks was available for training. 
The test set consisted of $379$ images, of which $75$ were of melanomas. 
All images are publicly available from the ISIC web site. 
To participate in the challenge, a posterior probability of malignancy for each image in the test set had to be submitted. 
To calculate the performance of the different systems, the following definitions were used: TP = true positive, FP = false positive, TN = true negative, and FN = false negative, where 'malignant' is positive and 'benign' is negative. 
Precision = $TP/(TP+FP)$, recall = sensitivity (SE) = $TP/(TP+FN)$, and specificity = $TN/(TN+FP)$. 
Two global measures of performance were calculated: area under the precision-recall curve (average precision), and area under the curve of the receiver-operating characteristic (AUC of ROC). 
In addition, specificity scores at sensitivities equal to $95\%$, $98\%$ and $99\%$ were calculated. 
We will refer to the latter ones as high-sensitivity measures. 

\subsection{Nevus Doctor} \label{sec:NevusDoctor}

Nevus Doctor is a computer system for detection of melanoma and non-melanocytic skin cancer in dermoscopic images. 
A detailed description can be found in \cite{Mollersen2015Improved} and \cite{Zortea2014Performance}. 
The system consists of automatic segmentation, feature extraction, and classification. 

For participation in the challenge, semi-automatic feature selection was conducted on the basis of the $900$ training images, following the procedure described in \cite{Mollersen2015Improved}. 
Apart from the $59$ features listed in \cite{Mollersen2015Improved}, three new features, described in \cite{Mollersen2015Divergencebased}, were added to the feature pool. 
Two classifiers were considered: linear discriminant analysis (LDA) and quadratic discriminant analysis (QDA). 
The feature and classifier selection resulted in $16$ features and the QDA classifier. 
Description of each feature can be found in \cite{Mollersen2015Improved} and in \cite{Zortea2014Performance} (feature f1, f5, f6, f10, f11, f12, f16, f17, f18, f54, f55, f56, f57, and f58), and in  \cite{Mollersen2015Divergencebased} (feature f60 and f62).

The prior class probability in a discriminant analysis classifier can be estimated based on the class labels in the training set. The output of the classifier is the posterior probability of an observation belonging to a certain class. For calculation of sensitivity values from 0\% to 100\%, the threshold for the posterior probability is adjusted from 0 to 1. Another option is to adjust the prior probability for the class from 0 to 1. 

For this paper, we have recalculated the high-sensitivity measures by adjusting the prior probability to avoid the situation where several lesions have the saturated posterior probability of $1.00$. 

\section{Results} \label{sec:Results}

The participants were ranked according to diagnostic accuracy of the test set. 
Other aspects such as computation time were not considered.
The challenge's ranking criterion was average precision, but scores for the other performance measures listed in Section~\ref{sec:Challenge} were provided as well.  
This made it possible to rank the participants according to several criteria, not only the one chosen by the challenge. 
Average precision and AUC of ROC are global performance measures, from $0\%$ sensitivity (no melanomas detected), to $100\%$ sensitivity (all melanomas detected). 
The high-sensitivity measures provide a performance measures given that a high proportion of the melanomas are detected. 

For those participants that were ranked highest by one of the measures, Table~\ref{tab:Rankings} shows the rankings
by the remaining measures. 
The score is given for the highest ranked participants. 
Descriptions of the algorithms used were not submitted for the challenge, and therefore the participants are only given a letter as identifier in the table. 
Participants B and D did not participate in the Manual segmentation part. 

\begin{table}[h!]
\begin{tabular}{c | c | c | c | c | c | c| c || c | c | c | c | c}
        Segmentation & \multicolumn{7}{c ||}{Automatic} & \multicolumn{5}{c}{Manual} \\
        \hline
        Participant & A & B & C & C & D & E & F & A & E & C & E & F \\
        \hline 
   Av. precision 	&  0.64 &  3 & 11 & 11 & 2 & 6 & 8 & 0.62 & 3 & 7 & 3 & 2   \\
  AUC of ROC   	&  3 &  0.83 &  6  &  6 &  4 & 8 & 5 & 4 &  0.81 & 5 & 1 & 2 \\
  SE = 95\%            	& 11 &  8 &  0.39 & 1  & 6  & 13 & 7 &  11 & 2 & 0.32 & 2 & 3 \\
  SE = 98\% 		& 15 & 14 & 1 &  0.33 & 4 & 13 & 5 & 11 & 1 & 5 & 0.29 & 2 \\
  SE = 99\% 		& 11 &   9 &  3 &  3 &  0.25 & 12 & 10 & 8 & 7 & 2 & 7  & 0.25 
\end{tabular}
  \caption{Rankings for those participants that were highest ranked by one measure.}
  \label{tab:Rankings}
\end{table}

From Table~\ref{tab:Rankings}, it is clear that choice of performance measure has strong impact on the resulting ranking.  
Participant A, which were ranked highest for both Automatic and Manual segmentation by the challenge, is ranked in the middle for the high-sensitivity measures. 

\subsection{The impact of segmentation}

From Table~\ref{tab:Rankings} we see that the ranking varies according to whether the participants use their own segmentation algorithm or the semi-automatic/manual segmentation provided by the challenge. 
Interestingly, the best score for each measure does not improve by semi-automatic/manual segmentation. 
This could mean that the automatic segmentation algorithms give approximately the same masks as the semi-automatic/manual segmentation, or it could mean that the segmentation mask does not influence the final classification significantly.

The algorithms for the challenge participants were not submitted, and therefore we investigated the impact of segmentation using the Nevus Doctor segmentation algorithm. 
Fig.~\ref{fig:Border} shows an example of the automatic segmentation used in Nevus Doctor and the manual segmentation provided by the challenge. 
In general, when the difference between the automatic and semi-automatic/manual segmentation was large, the automatic segmentation missed outer lighter areas of the lesion, like in Fig.~\ref{fig:Border}. 
\begin{figure}[h!]
     \includegraphics[width = 0.85 \textwidth]{/Volumes/kam025/Documents/Figures/ISBI_Results/ISIC_0000276_border.jpeg}
      \caption{Blue line: automatic segmentation. Red line: manual segmentation.}
       \label{fig:Border}
\end{figure}

The occasionally failed segmentation does not seem to have an impact on the classification result.
Fig.~\ref{fig:ROC} shows the ROC curves for automatic and semi-automatic/manual segmentation for Nevus Doctor. 
  \begin{figure}[h!]
     \includegraphics[width = 0.85 \textwidth]{/Volumes/kam025/Documents/Figures/ISBI_Results/ISBI_ROC_qda.jpg}
      \caption{ROC curves for skin lesion classification with automatic and semi-automatic/manual segmentation.}
       \label{fig:ROC}
   \end{figure}
The two curves follow each other closely for the whole range of sensitivities/specificities. 
This is also confirmed by the results for the five performance measures, as seen in Table~\ref{tab:ND_Rankings}.    
\begin{table}[h!]
\begin{tabular}{c | c | c | c | c}
        Segmentation: &{Automatic} & {Manual} \\
        \hline
  Average precision & 0.35 & 0.37 \\
  AUC of the ROC & 0.64 & 0.67 \\
  SE = 95\% & 0.23 & 0.19 \\
  SE = 98\%  & 0.13 & 0.17 \\
  SE = 99\%  & 0.12  & 0.17
\end{tabular}
  \caption{Scores for Nevus Doctor.}
  \label{tab:ND_Rankings}
\end{table}

\subsection{The impact of classifier} 

Because the segmentation algorithm had lesser impact than expected, we investigated the impact of classifier algorithm.
The performance of a classifier is tightly connected to the characteristics of the features, and it is therefore difficult to rank classifiers.
The differences in performance can be the result of the features being more suited to a certain classifier, and the ranking might change completely if other features are made available. 

We therefore investigate the performance of a few fairly similar classifiers. 
The aim is not to rank the classifiers for melanoma detection, but to investigate how the performance of a system changes by using a different classifier. 

When training Nevus Doctor for the challenge, LDA and QDA classifiers were considered, and QDA was used on the test set. 
In addition, we have investigated the diagonal LDA and QDA (dLDA and dQDA) classifiers, which correspond to naive Bayes classifiers. 
The features were not reselected, and therefore, QDA has a slight benefit. 
The results are shown in Table~\ref{tab:Diagonal}. 
\begin{table}
\begin{tabular}{c | c | c | c | c}
        \multicolumn{5}{c}{Automatic segmentation} \\
        \hline
        & LDA & QDA & dLDA & dQDA \\
        \hline
   SE = 95\% & 0.14 & 0.23 & 0.28 &0.32\\
  SE = 98\% & 0.07 & 0.13 & 0.22 & 0.27 \\
  SE = 99\% & 0.05 & 0.12 & 0.21 & 0.24 
\end{tabular}
  \caption{High-sensitivity measures for Nevus Doctor using different classifiers.}
  \label{tab:Diagonal}
\end{table}
We see that the performance varies considerably more between different classifiers than between the two segmentation methods. 

\section{Discussion} \label{sec:Discussion} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The ISIC data repository, consisting of more than $12,000$ dermoscopic images, has answered to a need expressed for several years in the field of computer aided melanoma detection: a publicly available data set. 
Two important outcomes of collecting and sharing data are: 
(1) Large data sets allow for large training sets and independent test sets. (2) Publicly availability allows for comparison of algorithms on the same data.
The ISBI challenge has taken advantage of this and set aside an independent test set, and ranked its participants according to diagnostic accuracy. 
However, we see from Table~\ref{tab:Rankings} that the choice of performance measure has great impact on the resulting ranking. 

In this challenge, the systems were ranked according to average precision, which weights misclassifications equally whether it is a melanoma misclassified as benign, or a benign lesion misclassified as malignant. 
In two large clinical studies, the specificity at $97\%$ sensitivity \citep{Malvehy2014Clinical} and $98\%$ sensitivity \citep{Monheit2011Performance} was used as performance measures, arguing that a melanoma misclassified as benign can have fatal outcome for the patient. 
A problem with ranking by very high sensitivity is its dependence on the data set. 
In this challenge, $99\%$ sensitivity corresponds to misclassifying only one melanoma, and under such circumstances, the ranking can be somewhat arbitrary. 
To avoid this, cross-validation, bootstrapping or similar methods can be used.   

From Table~\ref{tab:Rankings}, Table~\ref{tab:ND_Rankings} and Fig.~\ref{fig:ROC} we see that whether automatic or semi-automatic/manual segmentation is used had little impact on the final classification accuracy.
This imply that further improvement of automatic segmentation algorithms, when compared to semi-automatic/manual segmentation,  will not improve the classification accuracy.
However, manual segmentation does not necessarily reflect the true borders of the lesion.
Improvement of automatic segmentation algorithms when tested against classification accuracy could therefore improve the performance of a system. 

In Table~\ref{tab:Diagonal}, fairly similar classifiers, using the same set of features, give more variation in the classification accuracy than different segmentation methods. 
Although this does not imply that other systems have the same variation in performance, it is an indication that choice of classifier might have bigger impact than suggested earlier.  
All four classifiers have similar assumptions regarding the distribution of the feature values. 
The features were selected specifically for the QDA classifier, but dLDA and dQDA outperformed QDA for the high-sensitivity measures. 
Earlier studies are inconclusive {\color{red} Magliogannis (117), Dreiseitl (134), see Korotkov}.
Separating the impact of the classifier from that the features and feature selection is a difficult task.

An important question when analysing the diagnostic accuracy, whatever performance measure, is its generalisability to other data sets. 
Ideally, the results should reflect a system's performance under actual clinical conditions. 
The test set in this challenge was not consecutively collected, and inclusion and exclusion criteria are not stated (e.g. if non-melanoma skin cancers are excluded). Even though the performance in absolute numbers, e.g. specificity at $95\%$ sensitivity, are better for many of the algorithms in this challenge compared to other studies, one cannot draw the conclusion that these algorithms are in general better for that performance measure. 

%The major drawback of the ISIC Data Archive is the lack of clinical information about each sub data set and each lesion. 
%Of crucial importance are the inclusion and exclusion criteria for each sub data set (e.g. if non-melanoma skin cancers are excluded), and the Breslow thickness or the invasion level of the melanomas. 

Demanding more information can result in fewer deposits to the repository, but to have clinical impact, a minimum of clinical information is essential. 
See e.g., \citep{Malvehy2007Dermoscopy} for recommendations. 
 


%See e.g. \citet{Dreiseitl2009Computer}; or compare \citet{Elbaum2001Automatic} to \citet{Monheit2011Performance}; \citet{Bauer2000Digital} to \citet{Piccolo2002Dermoscopic}; \citet{Hoffmann2003Diagnostic} to \citet{Barzegari2005Computeraided} and \citet{Boldrick2007Evaluation}; and \citet{Blum2004Value} to \citet{Fruhauf2012Patient} and \citet{FueyoCasado2009Evaluation}. These are studies that report sensitivity and specificity scores first by training and testing on the same data, and later on independent test sets. 

A benefit of a public repository is variability in the data. 
The ISIC Data Archive consists of images taken with different cameras and dermoscopes and from different sources. 
This introduces some obstacles because the size and colour of a lesion have diagnostic importance, but are not consistent between different cameras and dermoscopes. 
However, testing a system on images from different cameras and dermoscopes is more realistic than using images from identical equipment. 
The more specific the requirements of the input are, the less versatile the computer system will be, and probably it will be less used. 
Additionally, camera and dermoscope technologies are developing fast, and a system must have the ability to easily adapt to new equipment, without an extensive training set. 
Different image resolutions, magnifications and distances to the skin result in the true size of the lesion differing from image to image, though this can be solved easily by providing information regarding pixels per mm. 
Additional aspects that pose a challenge are the different colour calibrations of the cameras and different light sources, as illustrated in Fig.~\ref{fig:Colour}. 
  \begin{figure}[h!]
  \centering
     \subfigure{\includegraphics[width = 0.32 \textwidth]{/Volumes/kam025/Documents/Figures/ISBI_Results/ISIC_0000023.jpg}}
     \subfigure{\includegraphics[width = 0.32 \textwidth]{/Volumes/kam025/Documents/Figures/ISBI_Results/ISIC_0000037.jpg}}
        \caption{Example of different colour calibration and/or different light source from the ISIC Data Archive.}
        \label{fig:Colour}
   \end{figure}
Colour is one of the main clinical features for melanoma detection \citep{Argenziano2003Dermoscopy}, and computer systems normally have one or more colour features \citep{Korotkov2012Computerized}. 
There have been attempts at automatic colour calibration based on image content \citep{Iyatomi2011Automated}, but the impact on classification is not known. 

% In a public repository, information on camera model and type of dermoscope can add quality to the data set.   

A computer system for melanoma detection can only provide decision support, not actual diagnosis, as long as the diagnostic accuracy is considerably lower than the current gold standard (histopathology). The usefulness of a system relies not only on its ability for correct classification but also on the possibility for the user to take advantage of the output. 

Nevus Doctor is a classical melanoma detection system in the sense that it follows the three main steps of segmentation, feature extraction and classification, which correspond to the three sub challenges.  
For the recent years, deep convolutional neural networks (CNN) have entered the field of image classification with great success, and has also been applied to melanoma detection \cite{Esteva2017Dermatologistlevel}. 
In the study of Esteva, CNN's performance was comparable to that of dermatologistse, which is the same as reported for classical melanoma detection systems.
A direct comparison between methods can give an answer to whether one set of methods outperforms the other. 
The ISBI 2017 melanoma detection challenge has added an additional requirement of submitting a description of the algorithms used, and a comparison between methods can be done once the results are published. 

\subsection{Conclusion}

The {\it Skin Lesion Analysis towards Melanoma Detection} challenge has resulted in the largest comparison between computer systems for dermoscopic images, both in number of participants and in test set size. 
Whereas the obstacle of small datasets and non-comparable results are solved with a public repository and an open challenge, the question of performance measure still remains. 
From a clinical perspective, the misclassification of a melanoma as benign has far greater cost than the misclassification of a benign lesion as malignant. 
A high-sensitivity measure can therefore improve the clinical relevance of the challenge, and help ISIC to achieve its goal ``to help reduce melanoma mortality'' (http://isdis.net/isic-project/). 
A system with $95\%$ sensitivity misclassifies one in 20 melanomas, and it is difficult to imagine that lower sensitivity is acceptable. 
The exact sensitivity score will be subject to discussion, even among dermatologists. 
To truly know whether a system can have clinical impact, it must be tested in a clinical setting, which is not feasible in an open challenge. 


%Semi-automatic/manual segmentation did not improve the classification accuracy compared with automatic segmentation for the six top ranked participants and for Nevus Doctor. 
%Further improvement in segmentation should therefore use another measure of success than resemblance to manual segmentation, perhaps its impact on the final classification.

%The classical melanoma detection systems, where segmentation, feature extraction and classification are performed as separate tasks, might get competition from CNN systems in the near future.
%As long as a system is used as decision support, not for actual diagnosis, output other than the class label can be of great value to the clinician. 
%In that perspective, classical systems with their hand-crafted features and simple classification algorithms have a benefit over the CNN systems. 
%
%


\bibliography{/Users/kam025/kajsam}



%\begin{table}
%\begin{tabular}{c | c | c | c | c | c || c | c }
%        \multicolumn{1}{c}{Segmentation}: & \multicolumn{5}{c }{  Automatic}& \multicolumn{2}{c }{Manual} \\
%        \hline
%        & LDA & QDA & dLDA & dQDA & Best & dQDA & Best \\
%        \hline
%   SE = 95\% & 0.14 & 0.23 & 0.28 &0.32 &0.39 & 0.32 & 0.32\\
%  SE = 98\% & 0.07 & 0.13 & 0.22 & 0.27 & 0.33 & 0.29 & 0.29\\
%  SE = 99\% & 0.05 & 0.12 & 0.21 & 0.24 & 0.25 & 0.27 & 0.25
%\end{tabular}
%  \caption{High-sensitivity measures for Nevus Doctor using different classifiers, compared to the best score among all participants.}
%  \label{tab:ND_Diagonal}
%\end{table}




\end{document}

