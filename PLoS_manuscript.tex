% Template for PLoS
% Version 3.4 January 2017
%
% % % % % % % % % % % % % % % % % % % % % %
%
% -- IMPORTANT NOTE
%
% This template contains comments intended 
% to minimize problems and delays during our production 
% process. Please follow the template instructions
% whenever possible.
%
% % % % % % % % % % % % % % % % % % % % % % % 
%
% Once your paper is accepted for publication, 
% PLEASE REMOVE ALL TRACKED CHANGES in this file 
% and leave only the final text of your manuscript. 
% PLOS recommends the use of latexdiff to track changes during review, as this will help to maintain a clean tex file.
% Visit https://www.ctan.org/pkg/latexdiff?lang=en for info or contact us at latex@plos.org.
%
%
% There are no restrictions on package use within the LaTeX files except that 
% no packages listed in the template may be deleted.
%
% Please do not include colors or graphics in the text.
%
% The manuscript LaTeX source should be contained within a single file (do not use \input, \externaldocument, or similar commands).
%
% % % % % % % % % % % % % % % % % % % % % % %
%
% -- FIGURES AND TABLES
%
% Please include tables/figure captions directly after the paragraph where they are first cited in the text.
%
% DO NOT INCLUDE GRAPHICS IN YOUR MANUSCRIPT
% - Figures should be uploaded separately from your manuscript file. 
% - Figures generated using LaTeX should be extracted and removed from the PDF before submission. 
% - Figures containing multiple panels/subfigures must be combined into one image file before submission.
% For figure citations, please use "Fig" instead of "Figure".
% See http://journals.plos.org/plosone/s/figures for PLOS figure guidelines.
%
% Tables should be cell-based and may not contain:
% - spacing/line breaks within cells to alter layout or alignment
% - do not nest tabular environments (no tabular environments within tabular environments)
% - no graphics or colored text (cell background color/shading OK)
% See http://journals.plos.org/plosone/s/tables for table guidelines.
%
% For tables that exceed the width of the text column, use the adjustwidth environment as illustrated in the example table in text below.
%
% % % % % % % % % % % % % % % % % % % % % % % %
%
% -- EQUATIONS, MATH SYMBOLS, SUBSCRIPTS, AND SUPERSCRIPTS
%
% IMPORTANT
% Below are a few tips to help format your equations and other special characters according to our specifications. For more tips to help reduce the possibility of formatting errors during conversion, please see our LaTeX guidelines at http://journals.plos.org/plosone/s/latex
%
% For inline equations, please be sure to include all portions of an equation in the math environment.  For example, x$^2$ is incorrect; this should be formatted as $x^2$ (or $\mathrm{x}^2$ if the romanized font is desired).
%
% Do not include text that is not math in the math environment. For example, CO2 should be written as CO\textsubscript{2} instead of CO$_2$.
%
% Please add line breaks to long display equations when possible in order to fit size of the column. 
%
% For inline equations, please do not include punctuation (commas, etc) within the math environment unless this is part of the equation.
%
% When adding superscript or subscripts outside of brackets/braces, please group using {}.  For example, change "[U(D,E,\gamma)]^2" to "{[U(D,E,\gamma)]}^2". 
%
% Do not use \cal for caligraphic font.  Instead, use \mathcal{}
%
% % % % % % % % % % % % % % % % % % % % % % % % 
%
% Please contact latex@plos.org with any questions.
%
% % % % % % % % % % % % % % % % % % % % % % % %

\documentclass[10pt,letterpaper]{article}
\usepackage[top=0.85in,left=2.75in,footskip=0.75in]{geometry}

% amsmath and amssymb packages, useful for mathematical formulas and symbols
\usepackage{amsmath,amssymb}

% Use adjustwidth environment to exceed column width (see example table in text)
\usepackage{changepage}

% Use Unicode characters when possible
\usepackage[utf8x]{inputenc}

% textcomp package and marvosym package for additional characters
\usepackage{textcomp,marvosym}

% cite package, to clean up citations in the main text. Do not remove.
\usepackage{cite}

% Use nameref to cite supporting information files (see Supporting Information section for more info)
\usepackage{nameref,hyperref}

% line numbers
\usepackage[right]{lineno}

% ligatures disabled
\usepackage{microtype}
\DisableLigatures[f]{encoding = *, family = * }

% color can be used to apply background shading to table cells only
\usepackage[table]{xcolor}

% array package and thick rules for tables
\usepackage{array}

% create "+" rule type for thick vertical lines
\newcolumntype{+}{!{\vrule width 2pt}}

% create \thickcline for thick horizontal lines of variable length
\newlength\savedwidth
\newcommand\thickcline[1]{%
  \noalign{\global\savedwidth\arrayrulewidth\global\arrayrulewidth 2pt}%
  \cline{#1}%
  \noalign{\vskip\arrayrulewidth}%
  \noalign{\global\arrayrulewidth\savedwidth}%
}

% \thickhline command for thick horizontal lines that span the table
\newcommand\thickhline{\noalign{\global\savedwidth\arrayrulewidth\global\arrayrulewidth 2pt}%
\hline
\noalign{\global\arrayrulewidth\savedwidth}}


% Remove comment for double spacing
%\usepackage{setspace} 
%\doublespacing

% Text layout
\raggedright
\setlength{\parindent}{0.5cm}
\textwidth 5.25in 
\textheight 8.75in

% Bold the 'Figure #' in the caption and separate it from the title/caption with a period
% Captions will be left justified
\usepackage[aboveskip=1pt,labelfont=bf,labelsep=period,justification=raggedright,singlelinecheck=off]{caption}
\renewcommand{\figurename}{Fig}

% Use the PLoS provided BiBTeX style
\bibliographystyle{plos2015}

% Remove brackets from numbering in List of References
\makeatletter
\renewcommand{\@biblabel}[1]{\quad#1.}
\makeatother

% Leave date blank
\date{}

% Header and Footer with logo
\usepackage{lastpage,fancyhdr,graphicx}
\usepackage{epstopdf}
\pagestyle{myheadings}
\pagestyle{fancy}
\fancyhf{}
\setlength{\headheight}{27.023pt}
\lhead{\includegraphics[width=2.0in]{PLOS-submission.eps}}
\rfoot{\thepage/\pageref{LastPage}}
\renewcommand{\footrule}{\hrule height 2pt \vspace{2mm}}
\fancyheadoffset[L]{2.25in}
\fancyfootoffset[L]{2.25in}
\lfoot{\sf PLOS}

%% Include all macros below

\newcommand{\lorem}{{\bf LOREM}}
\newcommand{\ipsum}{{\bf IPSUM}}

%% END MACROS SECTION


\begin{document}
\vspace*{0.2in}

% Title must be 250 characters or less.
\begin{flushleft}
{\Large
\textbf\newline{Comparison of computer systems and ranking criteria for automatic melanoma detection in dermoscopic image} % Please use "sentence case" for title and headings (capitalize only the first word in a title (or heading), the first word in a subtitle (or subheading), and any proper nouns).
}
\newline
% Insert author names, affiliations and corresponding author email (do not include titles, positions, or degrees).
\\
Kajsa M{\o}llersen\textsuperscript{1*},
Maciel Zortea\textsuperscript{2},
Thomas R. Schopf\textsuperscript{3},
Herbert Kirchesch\textsuperscript{4},
Fred Godtliebsen\textsuperscript{2}
\\
\bigskip
\textbf{1} Department of Community Medicine, UiT The Arctic University of Norway, Troms{\o}, Norway
\\
\textbf{2} Department of Mathematics and Statistics, UiT The Arctic University of Norway, Troms{\o}, Norway
\\
\textbf{3} Norwegian Centre for E-health Research, University Hospital of North Norway, Troms{\o}, Norway
\\
\textbf{4} Private office, Venloer Stra{\ss}e 107, 50259 Pulheim, Germany
\\
\bigskip

% Insert additional author notes using the symbols described below. Insert symbol callouts after author names as necessary.
% 
% Remove or comment out the author notes below if they aren't used.
%
% Primary Equal Contribution Note
% \Yinyang These authors contributed equally to this work.

% Additional Equal Contribution Note
% Also use this double-dagger symbol for special authorship notes, such as senior authorship.
% \ddag These authors also contributed equally to this work.

% Current address notes
% \textcurrency Current Address: Dept/Program/Center, Institution Name, City, State, Country % change symbol to "\textcurrency a" if more than one current address note
% \textcurrency b Insert second current address 
% \textcurrency c Insert third current address

% Deceased author note
% \dag Deceased

% Group/Consortium Author Note
% \textpilcrow Membership list can be found in the Acknowledgments section.

% Use the asterisk to denote corresponding authorship and provide email address in note below.
* kajsa.mollersen@uit.no

\end{flushleft}
% Please keep the abstract below 300 words
\section*{Abstract}
Melanoma is the deadliest form of skin cancer, and early detection is crucial for patient survival. 
Computer systems can assist in melanoma detection, but are not widespread in clinical practice. 
In 2016, an open challenge in classification of dermoscopic images of skin lesions was announced. 
Nevus Doctor, a computer system previously developed by the authors, was used to participate in the challenge.
We compare five different measures for diagnostic accuracy by analysing the resulting ranking of the computer systems in the challenge. 
The different measures are discussed from a clinical perspective. 

The unexpected small variation in diagnostic accuracy according to segmentation method is investigated through comparison of an automatic versus the semi-automatic/manual segmentation. 
A small set of similar classification algorithms are used to investigate the impact of classifier on the diagnostic accuracy. 

A training set of $900$ dermoscopic images with corresponding class labels was released for the challenge. 
An independent test set of $379$ images, of which $75$ were of melanomas, was used to rank the participants. 

Choice of performance measure had great impact on the ranking of computer systems. 
Systems that were ranked among the top three for one measure, dropped to the bottom half when changing performance measure. 
From a clinical perspective, the misclassification of a melanoma as benign has far greater cost than the misclassification of a benign lesion as malignant. 
For a computer system to have clinical impact, its performance should be measured in a clinical relevant way. 

The small impact of segmentation method on diagnostic accuracy either suggests that the automatic segmentation algorithms do not need improvement, or that improvement should be measured against diagnostic accuracy rather than resemblance to manual segmentation. 
The variability in diagnostic accuracy for different classifier algorithms was larger than the variability for segmentation methods, and suggests a focus for future investigations. 

% Please keep the Author Summary between 150 and 200 words
% Use first person. PLOS ONE authors please skip this step. 
% Author Summary not valid for PLOS ONE submissions.   
%\section*{Author summary}
%Lorem ipsum dolor sit amet, consectetur adipiscing elit. Curabitur eget porta erat. Morbi consectetur est vel gravida pretium. Suspendisse ut dui eu ante cursus gravida non sed sem. Nullam sapien tellus, commodo id velit id, eleifend volutpat quam. Phasellus mauris velit, dapibus finibus elementum vel, pulvinar non tellus. Nunc pellentesque pretium diam, quis maximus dolor faucibus id. Nunc convallis sodales ante, ut ullamcorper est egestas vitae. Nam sit amet enim ultrices, ultrices elit pulvinar, volutpat risus.

\linenumbers

% Use "Eq" instead of "Equation" for equation citations.
\section*{Introduction}
Melanoma is the deadliest form of skin cancer. The survival rate drops as the level of tumour invasion increases, and therefore early detection is crucial for the patient's survival \cite{AmericanCancerSociety2016Cancer, CancerRegistryofNorway2016Cancer}. Melanoma detection relies on visual inspection, which is challenging because melanomas often resemble benign skin lesions. 
An aid in melanoma detection is the dermoscope, a simple device consisting of a magnifying lens and surrounding light, which is commonly used by dermatologists to examine skin conditions. 
The dermoscope reveals structures in lesions not visible to the naked eye, and has proven to increase diagnostic accuracy of skin lesions when used by trained personnel \cite{Kittler2002Diagnostic}.

Computer systems for melanoma detection seek to correctly classify a lesion based on input and previous training. 
The input is typically a dermoscopic image, but other technologies such as multispectral imaging and Rahman spectroscopy are also used. 
See e.g., \cite{Fink2016Noninvasive} and \cite{Masood2013Computer} for an overview. 
Training relies on an available database of labelled lesion images. 

Research and development of computer systems for melanoma detection have increased in the past few decades, following the development of digital cameras and attachable dermoscopes. 
An overview can be found in \cite{Korotkov2012Computerized}. 
Despite the effort made in developing new systems, their use in clinical practice is not widespread. 
An explanation may be the lack of convincing reports on the systems' diagnostic accuracy, owing to small data sets and non-comparable results. 

Small data sets result in training and testing on the same data, often by incorrect use of cross-validation. 
This gives overly optimistic estimates of the test error \cite{Smialowski2010Pitfalls,Hastie2009Elements}, and the classification accuracy of the system drops dramatically when tested on an independent test set. 
In the studies that report independent testing, the test set is often too small for the results to be generalised. 
In addition, consecutively collection of images is crucial for a clinical-like dataset. 
Only two studies that we know of have more than $50$ melanomas in an independent test set of consecutively collected images \cite{Monheit2011Performance, Malvehy2014Clinical}. 

The diagnostic difficulty of the data set has significant impact on the reported diagnostic accuracy of the system, as discussed in \cite{Rosado2003Accuracy}, and illustrated in \cite{Malvehy2014Clinical} and \cite{Mollersen2015Computeraided}. 
The diagnostic difficulty of a data set depends on factors such as proportion of melanomas, exclusion of non-melanocytic lesions, median Breslow depth, etc. 
Therefore, different systems cannot be compared unless they are tested on the same data set. 
There are few studies where several systems are tested on the same set of lesions, with the exception of \cite{Perrinaud2007Can} and \cite{Mollersen2015Computeraided}. 

A sufficiently large, publicly available data set makes independent testing and comparison of diagnostic accuracy between systems possible. 
In 2013, the PH$^2$ data set was made publicly available \cite{Mendonca2013PH2}, and added important value to the research on computer systems for melanoma detection. 
Consisting of only $200$ images, it does not overcome the aforementioned obstacles of small data sets. 
A larger data set, the ISIC Data Archive (see Section~\ref{sec:Challenge}) is now publicly available. 
In 2016, an open challenge using images from the ISIC Data Archive was announced. 
Nevus Doctor, a computer system developed previously by the authors, was used to participate in this challenge.
The design of the challenge allows for a direct comparison of diagnostic accuracy among the submitted melanoma detection systems, since they have all been tested on the same independent test set. 
Even when systems are tested on the same test set, the question on how to measure the diagnostic accuracy arises. 
Summarising the performance of a system for the whole range of sensitivity and specificity values gives a different perspective than measuring the performance only for very high sensitivity values. 
Choice of measure is not trivial as different measures penalises misclassifications differently, and it can have huge impact on the ranking. 

The rest of the article is organised as follows: Section~\ref{sec:Challenge} describes the data, the challenge and the five performance measures. In addition, a brief description of Nevus Doctor is given. In Section~\ref{sec:Results}, the variations in ranking of the top systems are presented. An investigation into segmentation method and classifier algorithm is also found here. A discussion of the results is provided in Section~\ref{sec:Discussion}, together with some concluding remarks. 

\section*{The challenge} \label{sec:Challenge}

The International Skin Image Collaboration (ISIC) (http://isdis.net/isic-project/) has created a public repository for dermoscopic skin lesion images (https://isic-archive.com/). 
The current dataset (2016.03.12) consists of $12,086$ images and is by far the largest data set of dermoscopic images available. 
Each image is annotated as `benign' or `malignant' according to histopathological diagnosis or expert consensus based on revision of the image. 
The images come from different sources, and hence are taken with different cameras and dermoscopes. 

As part of the International Symposium on Biomedical Imaging (ISBI) 2016, the open challenge {\it Skin Lesion Analysis towards Melanoma Detection} was hosted by ISIC. 
The challenge consisted of three sub challenges: Segmentation, Feature Extraction, and Classification. 
Details can be found in \cite{Gutman2016Skin}. 
The Classification sub challenge had two parts: Automatic segmentation, where only the images were released, and Manual segmentation, where a semi-automatic or manually obtained mask was also released for each image. 
There were $25$ participants in the Automatic segmentation part, and $18$ participants in the Manual segmentation part. 
Many participated in both parts. 

A set of $900$ dermoscopic images of skin lesions with corresponding class labels and masks was available for training. 
The test set consisted of $379$ images, of which $75$ were of melanomas. 
All images are publicly available from the ISIC web site. 
To participate in the challenge, a posterior probability of malignancy for each image in the test set had to be submitted. 
To calculate the performance of the different systems, the following definitions were used: TP = true positive, FP = false positive, TN = true negative, and FN = false negative, where 'malignant' is positive and 'benign' is negative. 
Precision = $TP/(TP+FP)$, recall = sensitivity (SE) = $TP/(TP+FN)$, and specificity = $TN/(TN+FP)$. 
Two global measures of performance were calculated: area under the precision-recall curve (average precision), and area under the curve of the receiver-operating characteristic (AUC of ROC). 
In addition, specificity scores at sensitivities equal to $95\%$, $98\%$ and $99\%$ were calculated. 
We will refer to the latter ones as high-sensitivity measures. 

\subsection*{Nevus Doctor} \label{sec:NevusDoctor}

Nevus Doctor is a computer system for detection of melanoma and non-melanocytic skin cancer in dermoscopic images. 
A detailed description can be found in \cite{Mollersen2015Improved} and \cite{Zortea2014Performance}. 
The system consists of automatic segmentation, feature extraction, and classification. 

For participation in the challenge, semi-automatic feature selection was conducted on the basis of the $900$ training images, following the procedure described in \cite{Mollersen2015Improved}. 
Apart from the $59$ features listed in \cite{Mollersen2015Improved}, three new features, described in \cite{Mollersen2015Divergencebased}, were added to the feature pool. 
Two classifiers were considered: linear discriminant analysis (LDA) and quadratic discriminant analysis (QDA). 
The feature and classifier selection resulted in $16$ features and the QDA classifier. 
Description of each feature can be found in \cite{Mollersen2015Improved} and in \cite{Zortea2014Performance} (feature f1, f5, f6, f10, f11, f12, f16, f17, f18, f54, f55, f56, f57, and f58), and in  \cite{Mollersen2015Divergencebased} (feature f60 and f62).

The prior class probability in a discriminant analysis classifier can be estimated based on the class labels in the training set. The output of the classifier is the posterior probability of an observation belonging to a certain class. For calculation of sensitivity values from 0\% to 100\%, the threshold for the posterior probability is adjusted from 0 to 1. Another option is to adjust the prior probability for the class from 0 to 1. 
For this paper, we have recalculated the high-sensitivity measures by adjusting the prior probability to avoid the situation where several lesions have the saturated posterior probability of $1.00$. 


% For figure citations, please use "Fig" instead of "Figure".



% Results and Discussion can be combined.
\section*{Results} \label{sec:Results}
The participants were ranked according to diagnostic accuracy of the test set. 
Other aspects such as computation time were not considered.
The challenge's ranking criterion was average precision, but scores for the other performance measures listed in Section~\ref{sec:Challenge} were provided as well.  
This made it possible to rank the participants according to several criteria, not only the one chosen by the challenge. 
Average precision and AUC of ROC are global performance measures, from $0\%$ sensitivity (no melanomas detected), to $100\%$ sensitivity (all melanomas detected). 
The high-sensitivity measures provide a performance measures given that a high proportion of the melanomas are detected. 

For those participants that were ranked highest by one of the measures, Table~\ref{tab:Rankings} shows the rankings
by the remaining measures. 
The score is given for the highest ranked participants. 
Descriptions of the algorithms used were not submitted for the challenge, and therefore the participants are only given a letter as identifier in the table. 
Participants B and D did not participate in the Manual segmentation part. 
% Place tables after the first paragraph in which they are cited.
\begin{table}[!ht]
\begin{adjustwidth}{-0.25in}{0in} % Comment out/remove adjustwidth environment if table fits in text column.
\centering
\caption{
{\bf Rankings for those participants that were highest ranked by one measure.}}
\begin{tabular}{c | c | c | c | c | c | c| c || c | c | c | c | c}
        Segmentation & \multicolumn{7}{c ||}{Automatic} & \multicolumn{5}{c}{Manual} \\
        \hline
        Participant & A & B & C & C & D & E & F & A & E & C & E & F \\
        \hline 
   Av. precision 	&  0.64 &  3 & 11 & 11 & 2 & 6 & 8 & 0.62 & 3 & 7 & 3 & 2   \\
  AUC of ROC   	&  3 &  0.83 &  6  &  6 &  4 & 8 & 5 & 4 &  0.81 & 5 & 1 & 2 \\
  SE = 95\%            	& 11 &  8 &  0.39 & 1  & 6  & 13 & 7 &  11 & 2 & 0.32 & 2 & 3 \\
  SE = 98\% 		& 15 & 14 & 1 &  0.33 & 4 & 13 & 5 & 11 & 1 & 5 & 0.29 & 2 \\
  SE = 99\% 		& 11 &   9 &  3 &  3 &  0.25 & 12 & 10 & 8 & 7 & 2 & 7  & 0.25 
\end{tabular}
%\begin{flushleft} Table notes Phasellus venenatis, tortor nec vestibulum mattis, massa tortor interdum felis, nec pellentesque metus tortor nec nisl. Ut ornare mauris tellus, vel dapibus arcu suscipit sed.
%\end{flushleft}
\label{tab:Rankings}
\end{adjustwidth}
\end{table}

From Table~\ref{tab:Rankings}, it is clear that choice of performance measure has strong impact on the resulting ranking.  
Participant A, which were ranked highest for both Automatic and Manual segmentation by the challenge, is ranked in the middle for the high-sensitivity measures. 


%PLOS does not support heading levels beyond the 3rd (no 4th level headings).
\subsection*{The impact of segmentation}

From Table~\ref{tab:Rankings} we see that the ranking varies according to whether the participants use their own segmentation algorithm or the semi-automatic/manual segmentation provided by the challenge. 
Interestingly, the best score for each measure does not improve by semi-automatic/manual segmentation. 
This could mean that the automatic segmentation algorithms give approximately the same masks as the semi-automatic/manual segmentation, or it could mean that the segmentation mask does not influence the final classification significantly.

The algorithms for the challenge participants were not submitted, and therefore we investigated the impact of segmentation for Nevus Doctor. 
Fig~\ref{fig:Border} shows an example of the automatic segmentation used in Nevus Doctor and the manual segmentation provided by the challenge. 
In general, when the difference between the automatic and semi-automatic/manual segmentation was large, the automatic segmentation missed outer lighter areas of the lesion, like in Fig~\ref{fig:Border}. 

% Place figure captions after the first paragraph in which they are cited.
\begin{figure}[!h]
\caption{{\bf Segmentation by different methods.}
Blue line: automatic segmentation. Red line: manual segmentation.}
\label{fig:Border}
\end{figure}

The occasionally failed segmentation does not seem to have an impact on the classification result.
Fig~\ref{fig:ROC} shows the ROC curves for automatic and semi-automatic/manual segmentation for Nevus Doctor. 
\begin{figure}[!h]
\caption{{\bf ROC curves for different segmentation methods.}
ROC curves for skin lesion classification with automatic and semi-automatic/manual segmentation.}
\label{fig:ROC}
\end{figure}
The two curves follow each other closely for the whole range of sensitivities/specificities. 
This is also confirmed by the results for the five performance measures, as seen in Table~\ref{tab:ND_Rankings}.
\begin{table}[!ht]
% \begin{adjustwidth}{-0.25in}{0in} % Comment out/remove adjustwidth environment if table fits in text column.
\centering
\caption{
{\bf Scores for Nevus Doctor.}}
\begin{tabular}{c | c | c | c | c}
        Segmentation: &{Automatic} & {Manual} \\
        \hline
  Average precision & 0.35 & 0.37 \\
  AUC of the ROC & 0.64 & 0.67 \\
  SE = 95\% & 0.23 & 0.19 \\
  SE = 98\%  & 0.13 & 0.17 \\
  SE = 99\%  & 0.12  & 0.17
\end{tabular}
%\begin{flushleft} Table notes Phasellus venenatis, tortor nec vestibulum mattis, massa tortor interdum felis, nec pellentesque metus tortor nec nisl. Ut ornare mauris tellus, vel dapibus arcu suscipit sed.
%\end{flushleft}
\label{tab:ND_Rankings}
% \end{adjustwidth}
\end{table}

\subsection*{The impact of classifier algorithm} 

Because the segmentation algorithm had lesser impact than expected, we investigated the impact of classifier algorithm.
The performance of a classifier is tightly connected to the characteristics of the features, and it is therefore difficult to rank classifiers.
The differences in performance can be the result of the features being more suited to a certain classifier, and the ranking might change if other features are made available. 

We therefore investigate the performance of a few fairly similar classifiers. 
The aim is not to rank the classifiers for melanoma detection, but to investigate how the performance of a system changes by using a different classifier. 

When training Nevus Doctor for the challenge, LDA and QDA classifiers were considered, and QDA was used on the test set. 
In addition, we have investigated the diagonal LDA and QDA (dLDA and dQDA) classifiers, which correspond to naive Bayes classifiers. 
The features were not reselected, and therefore, QDA has a slight benefit. 
The results are shown in Table~\ref{tab:Diagonal}. 
\begin{table}[!ht]
% \begin{adjustwidth}{-0.25in}{0in} % Comment out/remove adjustwidth environment if table fits in text column.
\centering
\caption{
{\bf High-sensitivity measures for Nevus Doctor using different classifiers.}}
\begin{tabular}{c | c | c | c | c}
        \multicolumn{5}{c}{Automatic segmentation} \\
        \hline
        & LDA & QDA & dLDA & dQDA \\
        \hline
   SE = 95\% & 0.14 & 0.23 & 0.28 &0.32\\
  SE = 98\% & 0.07 & 0.13 & 0.22 & 0.27 \\
  SE = 99\% & 0.05 & 0.12 & 0.21 & 0.24 
\end{tabular}
%\begin{flushleft} Table notes Phasellus venenatis, tortor nec vestibulum mattis, massa tortor interdum felis, nec pellentesque metus tortor nec nisl. Ut ornare mauris tellus, vel dapibus arcu suscipit sed.
%\end{flushleft}
\label{tab:Diagonal}
% \end{adjustwidth}
\end{table}
We see that the performance varies considerably more between different classifiers than between the two segmentation methods. 


\section*{Discussion} \label{sec:Discussion} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The ISIC data repository, consisting of more than $12,000$ dermoscopic images, has answered to a need expressed for several years in the field of computer aided melanoma detection: a publicly available data set. 
Two important outcomes of collecting and sharing data are: 
(1) Large data sets allow for large training sets and independent test sets. (2) Publicly availability allows for comparison of algorithms on the same data.
The ISBI challenge has taken advantage of this and set aside an independent test set, and ranked its participants according to diagnostic accuracy. 
However, we see from Table~\ref{tab:Rankings} that the choice of performance measure has great impact on the resulting ranking. 

In this challenge, the systems were ranked according to average precision, which weights misclassifications equally whether it is a melanoma misclassified as benign, or a benign lesion misclassified as malignant. 
Specificity score at high sensitivity takes into account that a melanoma misclassified as benign can have fatal outcome for the patient, and has been used in clinical studies for melanoma detection \cite{Malvehy2014Clinical},\cite{Monheit2011Performance}.
A problem with ranking by very high sensitivity is its dependence on the data set. 
In this challenge, $99\%$ sensitivity corresponds to misclassifying only one melanoma, and under such circumstances, the ranking can be somewhat arbitrary. 
To avoid this, cross-validation, bootstrapping or similar methods can be used.   

From Table~\ref{tab:Rankings}, Table~\ref{tab:ND_Rankings} and Fig~\ref{fig:ROC} we see that whether automatic or semi-automatic/manual segmentation is used had little impact on the final classification accuracy.
This imply that further improvement of automatic segmentation algorithms, when compared to semi-automatic/manual segmentation,  will not improve the classification accuracy.
However, manual segmentation does not necessarily reflect the true borders of the lesion.
Improvement of automatic segmentation algorithms when tested against classification accuracy could therefore improve the performance of a system. 

In Table~\ref{tab:Diagonal}, fairly similar classifiers, using the same set of features, give more variation in the classification accuracy than different segmentation methods. 
Although this does not imply that other systems have the same variation in performance, it is an indication that choice of classifier might have bigger impact than suggested earlier \cite{Maglogiannis2009Overview}. 

An important question when analysing the diagnostic accuracy, whatever performance measure, is its generalisability to other data sets. 
Ideally, the results should reflect a system's performance under actual clinical conditions. 
The test set in this challenge was not consecutively collected, and inclusion and exclusion criteria are not stated (e.g. if non-melanoma skin cancers are excluded). Even though the performance in absolute numbers, e.g. specificity at $95\%$ sensitivity, are better for many of the algorithms in this challenge compared to other studies, one cannot any conclusions. 

Demanding more information can result in fewer deposits to the repository, but to have clinical impact, a minimum of clinical information is essential. 
See e.g., \cite{Malvehy2007Dermoscopy} for recommendations. 

A benefit of a public repository is variability in the data. 
The ISIC Data Archive consists of images taken with different cameras and dermoscopes, and from different sources. 
Testing a system on images from different cameras and dermoscopes is more realistic than using images from identical equipment. 
The more specific the requirements of the input are, the less versatile the computer system will be, and probably it will be less used. 
Additionally, camera and dermoscope technologies are developing fast, and a system must have the ability to easily adapt to new equipment, without an extensive training set. 
However, it introduces some obstacles because the size and colour of a lesion have diagnostic importance, but are not consistent between different cameras and dermoscopes. 
Different image resolutions, magnifications and distances to the skin result in the true size of the lesion differing from image to image, though this can be solved easily by providing information regarding pixels per mm. 
Additional aspects that pose a challenge are the different colour calibrations of the cameras and different light sources, as illustrated in Fig~\ref{fig:Colour}. 
\begin{figure}[!h]
\caption{{\bf Different colour}
Example of different colour calibration and/or different light source from the ISIC Data Archive.}
\label{fig:Colour}
\end{figure}
Colour is one of the main clinical features for melanoma detection \cite{Argenziano2003Dermoscopy}, and computer systems normally have one or more colour features \cite{Korotkov2012Computerized}. 
There have been attempts at automatic colour calibration based on image content \cite{Iyatomi2011Automated}, but the impact on classification is not known. 

For the recent years, deep convolutional neural networks (CNN) have entered the field of image classification with great success, and has also been applied to melanoma detection \cite{Esteva2017Dermatologistlevel}. 
In the study of Esteva, CNN's performance was comparable to that of dermatologists, which is the same as reported for classical melanoma detection systems.
A direct comparison between methods can give an answer to whether one set of methods outperforms the other. 
The ISBI 2017 melanoma detection challenge has added an additional requirement of submitting a description of the algorithms used, and a comparison between methods can be done once the results are published. 

A computer system for melanoma detection can only provide decision support, not actual diagnosis, as long as the diagnostic accuracy is considerably lower than the current gold standard (histopathology). 
The usefulness of a system relies not only on its ability for correct classification but also on the possibility for the user to take advantage of the output. 
To truly know whether a system can have clinical impact, it must be tested in a clinical setting, which is not feasible in an open challenge. 

\subsection*{Conclusion}

The {\it Skin Lesion Analysis towards Melanoma Detection} challenge has resulted in the largest comparison between computer systems for dermoscopic images, both in number of participants and in test set size. 
Whereas the obstacle of small datasets and non-comparable results are solved with a public repository and an open challenge, the question of performance measure still remains. 
From a clinical perspective, the misclassification of a melanoma as benign has far greater cost than the misclassification of a benign lesion as malignant. 
A system with $95\%$ sensitivity misclassifies one in 20 melanomas, and it is difficult to imagine that lower sensitivity is acceptable. 
The exact sensitivity score will be subject to discussion, even among dermatologists. 
A high-sensitivity measure can improve the clinical relevance of the challenge, and help ISIC to achieve its goal ``to help reduce melanoma mortality'' (http://isdis.net/isic-project/). 




%\section*{Acknowledgments}
%Cras egestas velit mauris, eu mollis turpis pellentesque sit amet. Interdum et malesuada fames ac ante ipsum primis in faucibus. Nam id pretium nisi. Sed ac quam id nisi malesuada congue. Sed interdum aliquet augue, at pellentesque quam rhoncus vitae.

\nolinenumbers

% Either type in your references using
% \begin{thebibliography}{}
% \bibitem{}
% Text
% \end{thebibliography}
%
% or
%
% Compile your BiBTeX database using our plos2015.bst
% style file and paste the contents of your .bbl file
% here. See http://journals.plos.org/plosone/s/latex for 
% step-by-step instructions.
% 
\bibliography{/Users/kam025/kajsam}


%\begin{thebibliography}{10}
%
%\bibitem{bib1}
%Conant GC, Wolfe KH.
%\newblock {{T}urning a hobby into a job: how duplicated genes find new
%  functions}.
%\newblock Nat Rev Genet. 2008 Dec;9(12):938--950.
%
%\bibitem{bib2}
%Ohno S.
%\newblock Evolution by gene duplication.
%\newblock London: George Alien \& Unwin Ltd. Berlin, Heidelberg and New York:
%  Springer-Verlag.; 1970.
%
%\bibitem{bib3}
%Magwire MM, Bayer F, Webster CL, Cao C, Jiggins FM.
%\newblock {{S}uccessive increases in the resistance of {D}rosophila to viral
%  infection through a transposon insertion followed by a {D}uplication}.
%\newblock PLoS Genet. 2011 Oct;7(10):e1002337.
%
%\end{thebibliography}



\end{document}

